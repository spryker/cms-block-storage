input {
  # Input: Spryker "lumberjack" log files
  file {
    path => "/data/shop/*/current/data/common/lumberjack/lumberjack-*.log"
    type => "spryker"
    start_position => "beginning"
  }

  # Input: Skryper exception/fatal log
  file {
    path => "/data/shop/*/current/data/*/logs/*/exception.log"
    type => "fatal"
    start_position => "beginning"
    codec => multiline {
      pattern => "^-------"
      negate => true
      what => previous
    }
  }

}

filter {
  # Filter: parse JSON from inputs, extract event timestamp from field
  if [type] == "spryker" {
    json {
      source => "message"
    }
    date {
      match => [ "dateAndTime", "ISO8601" ]
      timezone => "Etc/UTC"
    }
  }

  # Filter: extract php fatal errors, drop exceptions
  if [type] == "fatal" {
    grok {
      match => [ "message", ".*FATAL ERROR.*" ]
      tag_on_failure => [ "_not_fatal" ]
    }
    if "_not_fatal" in [tags] {
      drop {}
    }
  }
}

# Output: save direct in Elasticsearch instance for logs
# Optional for high-traffic sites: save to intermediate, quick store (like redis)
# and use seperate logstash instance to transfrom entries and ship them to Elasticsearch
output {
#  stdout { codec => rubydebug }
  elasticsearch_http {
    host => "localhost"
    port => 9200
    manage_template => "true"
    workers => "1"
    flush_size => "1000"
    idle_flush_time => "5"
    index => "logstash-%{+YYYY.MM.dd}"
  }
}
